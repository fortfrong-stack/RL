{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound-Based Navigation System in Google Colab\n",
    "\n",
    "This notebook demonstrates how to run the sound-based navigation system in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy torch matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create project structure and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "import os\n",
    "os.makedirs('core', exist_ok=True)\n",
    "os.makedirs('rl', exist_ok=True)\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('interface', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile core/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile core/grid_world.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.sound_source import Wall, SoundSource, propagate_sound\n",
    "from utils.audio_processing import get_audio_observation_features\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A 25x25 grid world for the sound-based navigation task.\n",
    "    Cell types: 0 - empty, 1 - wall, 2 - agent, 3 - sound source\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width: int = 25, height: int = 25):\n",
    "        \"\"\"\n",
    "        Initialize the grid world.\n",
    "        \n",
    "        Args:\n",
    "            width: Width of the grid (default 25)\n",
    "            height: Height of the grid (default 25)\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = np.zeros((height, width), dtype=np.int8)\n",
    "        \n",
    "        # Initialize agent and sound sources\n",
    "        self.agent_pos = None\n",
    "        self.sound_sources = []\n",
    "        self.wall_objects = []  # Store Wall objects instead of just coordinates\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the grid to initial state\"\"\"\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=np.int8)\n",
    "        self.agent_pos = None\n",
    "        self.sound_sources = []\n",
    "        self.wall_objects = []\n",
    "        \n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the current state of the grid.\n",
    "        \n",
    "        Returns:\n",
    "            Copy of the grid state\n",
    "        \"\"\"\n",
    "        return self.grid.copy()\n",
    "        \n",
    "    def render(self) -> None:\n",
    "        \"\"\"Visualize the current state of the grid\"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        # Create a copy of the grid for visualization\n",
    "        vis_grid = self.grid.copy()\n",
    "        \n",
    "        # Mark the agent position if exists\n",
    "        if self.agent_pos is not None:\n",
    "            x, y = self.agent_pos\n",
    "            vis_grid[x][y] = 2\n",
    "            \n",
    "        # Mark sound sources if exist\n",
    "        for source in self.sound_sources:\n",
    "            vis_grid[source.x][source.y] = 3\n",
    "            \n",
    "        plt.imshow(vis_grid, cmap='viridis', interpolation='nearest')\n",
    "        plt.colorbar(label='Cell Type (0: Empty, 1: Wall, 2: Agent, 3: Sound Source)')\n",
    "        plt.title('Grid World Visualization')\n",
    "        plt.show()\n",
    "        \n",
    "    def is_valid_position(self, x: int, y: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if position is within bounds.\n",
    "        \n",
    "        Args:\n",
    "            x: X coordinate\n",
    "            y: Y coordinate\n",
    "            \n",
    "        Returns:\n",
    "            True if position is valid, False otherwise\n",
    "        \"\"\"\n",
    "        return 0 <= x < self.width and 0 <= y < self.height\n",
    "        \n",
    "    def place_wall(self, x: int, y: int, permeability: float = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        Place a wall at the given position with permeability.\n",
    "        \n",
    "        Args:\n",
    "            x: X coordinate\n",
    "            y: Y coordinate\n",
    "            permeability: Permeability of the wall (default 0.5)\n",
    "        \"\"\"\n",
    "        if self.is_valid_position(x, y):\n",
    "            self.grid[x][y] = 1\n",
    "            wall = Wall(x, y, permeability)\n",
    "            self.wall_objects.append(wall)\n",
    "            \n",
    "    def place_sound_source(self, sound_source: 'SoundSource') -> None:\n",
    "        \"\"\"\n",
    "        Place a sound source on the grid.\n",
    "        \n",
    "        Args:\n",
    "            sound_source: SoundSource object to place\n",
    "        \"\"\"\n",
    "        if self.is_valid_position(sound_source.x, sound_source.y):\n",
    "            self.sound_sources.append(sound_source)\n",
    "            \n",
    "    def place_agent(self, x: int, y: int) -> bool:\n",
    "        \"\"\"\n",
    "        Place the agent at the given position.\n",
    "        \n",
    "        Args:\n",
    "            x: X coordinate\n",
    "            y: Y coordinate\n",
    "            \n",
    "        Returns:\n",
    "            True if placement was successful, False otherwise\n",
    "        \"\"\"\n",
    "        if self.is_valid_position(x, y) and self.grid[x][y] == 0:\n",
    "            self.agent_pos = (x, y)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def compute_sound_map(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the sound propagation map based on sources and walls.\n",
    "        \n",
    "        Returns:\n",
    "            2D numpy array representing sound intensity at each cell\n",
    "        \"\"\"\n",
    "        return propagate_sound(self.grid, self.sound_sources, self.wall_objects)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Basic agent that can move in the grid world.\n",
    "    Actions: up, down, left, right, stay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start_x: int = 0, start_y: int = 0):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            start_x: Starting X coordinate (default 0)\n",
    "            start_y: Starting Y coordinate (default 0)\n",
    "        \"\"\"\n",
    "        self.x = start_x\n",
    "        self.y = start_y\n",
    "        self.position = (start_x, start_y)\n",
    "        \n",
    "        # Define possible actions\n",
    "        self.actions = {\n",
    "            0: 'up',\n",
    "            1: 'down', \n",
    "            2: 'left',\n",
    "            3: 'right',\n",
    "            4: 'stay'\n",
    "        }\n",
    "        \n",
    "        # Direction vectors for movement\n",
    "        self.action_vectors = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1),\n",
    "            'stay': (0, 0)\n",
    "        }\n",
    "        \n",
    "    def move(self, action: int, grid_world: 'GridWorld') -> tuple:\n",
    "        \"\"\"\n",
    "        Move the agent according to the action in the given grid world.\n",
    "        \n",
    "        Args:\n",
    "            action: Action index (0-4) or action name ('up', 'down', etc.)\n",
    "            grid_world: GridWorld instance to move in\n",
    "            \n",
    "        Returns:\n",
    "            New position of the agent as (x, y) tuple\n",
    "        \"\"\"\n",
    "        # Handle numpy integers as well as Python integers\n",
    "        import numbers\n",
    "        if isinstance(action, numbers.Integral):\n",
    "            action = self.actions[action]\n",
    "            \n",
    "        if action not in self.action_vectors:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "            \n",
    "        dx, dy = self.action_vectors[action]\n",
    "        new_x = self.x + dx\n",
    "        new_y = self.y + dy\n",
    "        \n",
    "        # Check boundaries and collisions with walls\n",
    "        # We need to check the actual grid state, not a copy\n",
    "        if (grid_world.is_valid_position(new_x, new_y) and \n",
    "            grid_world.grid[new_x][new_y] != 1):  # Not a wall\n",
    "            self.x = new_x\n",
    "            self.y = new_y\n",
    "            self.position = (self.x, self.y)\n",
    "            \n",
    "            # Update the agent position in the grid world\n",
    "            grid_world.agent_pos = (self.x, self.y)\n",
    "            \n",
    "        return self.position\n",
    "        \n",
    "    def get_position(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Get current position of the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Current position as (x, y) tuple\n",
    "        \"\"\"\n",
    "        return self.position\n",
    "        \n",
    "    def observe(self, sound_map: np.ndarray = None, grid_world: 'GridWorld' = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Audio observation for the agent.\n",
    "        Returns audio features extracted from the sound at the agent's position.\n",
    "        \n",
    "        Args:\n",
    "            sound_map: Sound intensity map (optional)\n",
    "            grid_world: GridWorld instance (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Audio observation features as numpy array\n",
    "        \"\"\"\n",
    "        if sound_map is not None:\n",
    "            intensity = sound_map[self.x][self.y]\n",
    "            \n",
    "            # Determine frequency content from nearby sound sources\n",
    "            frequency_content = self._get_frequency_content_at_position(grid_world)\n",
    "            \n",
    "            # Get audio observation features\n",
    "            return get_audio_observation_features(intensity, frequency_content)\n",
    "        else:\n",
    "            # Return default audio features when no sound map is available\n",
    "            return get_audio_observation_features(0.0, 0.5)  # Default: no intensity, medium frequency\n",
    "\n",
    "    def _get_frequency_content_at_position(self, grid_world: 'GridWorld') -> float:\n",
    "        \"\"\"\n",
    "        Get the dominant frequency content at the agent's position based on nearby sources.\n",
    "        This considers both the distance and volume of sources to determine which one is loudest.\n",
    "        \n",
    "        Args:\n",
    "            grid_world: GridWorld instance to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dominant frequency content (0.0-1.0)\n",
    "        \"\"\"\n",
    "        if grid_world is None or not grid_world.sound_sources:\n",
    "            return 0.5  # Default frequency content\n",
    "\n",
    "        # Find the most prominent sound source at the agent's position based on perceived loudness\n",
    "        max_perceived_loudness = -1\n",
    "        dominant_frequency = 0.5\n",
    "        \n",
    "        for source in grid_world.sound_sources:\n",
    "            # Calculate Manhattan distance\n",
    "            distance = abs(self.x - source.x) + abs(self.y - source.y)\n",
    "            \n",
    "            # Calculate perceived loudness based on distance and source volume\n",
    "            # Using inverse relationship similar to our sound propagation\n",
    "            perceived_loudness = source.volume / (1 + 0.5 * distance + 0.1 * distance**1.5)\n",
    "            \n",
    "            if perceived_loudness > max_perceived_loudness:\n",
    "                max_perceived_loudness = perceived_loudness\n",
    "                dominant_frequency = source.frequency\n",
    "                \n",
    "        return dominant_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile core/sound_source.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Wall:\n",
    "    \"\"\"\n",
    "    Represents a wall in the grid world with permeability properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: int, y: int, permeability: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize a wall.\n",
    "        \n",
    "        Args:\n",
    "            x: X coordinate of the wall\n",
    "            y: Y coordinate of the wall\n",
    "            permeability: How much sound passes through the wall (0.0-1.0)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.permeability = permeability\n",
    "        \n",
    "        \n",
    "class SoundSource:\n",
    "    \"\"\"\n",
    "    Represents a sound source in the grid world.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: int, y: int, volume: float = 1.0, frequency: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize a sound source.\n",
    "        \n",
    "        Args:\n",
    "            x: X coordinate of the sound source\n",
    "            y: Y coordinate of the sound source\n",
    "            volume: Volume of the sound source (0.0-1.0)\n",
    "            frequency: Frequency characteristic of the sound (0.0-1.0)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.volume = volume\n",
    "        self.frequency = frequency\n",
    "        \n",
    "        \n",
    "def propagate_sound(grid, sound_sources, walls, decay_factor=0.5, spread_distance=5):\n",
    "    \"\"\"\n",
    "    Simulate sound propagation in the grid world considering walls.\n",
    "    \n",
    "    Args:\n",
    "        grid: 2D numpy array representing the grid world\n",
    "        sound_sources: List of SoundSource objects\n",
    "        walls: List of Wall objects\n",
    "        decay_factor: Factor determining how quickly sound decays with distance\n",
    "        spread_distance: Maximum distance sound can travel\n",
    "        \n",
    "    Returns:\n",
    "        2D numpy array representing sound intensity at each cell\n",
    "    \"\"\"\n",
    "    height, width = grid.shape\n",
    "    sound_map = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Create a map of wall positions and permeabilities\n",
    "    wall_map = np.ones((height, width), dtype=np.float32)  # Default: no obstruction\n",
    "    for wall in walls:\n",
    "        wall_map[wall.x, wall.y] = wall.permeability\n",
    "    \n",
    "    # For each sound source, propagate its sound\n",
    "    for source in sound_sources:\n",
    "        # Start with the source's volume at its position\n",
    "        sound_map[source.x, source.y] += source.volume\n",
    "        \n",
    "        # Propagate sound to neighboring cells\n",
    "        for dist in range(1, spread_distance + 1):\n",
    "            for dx in range(-dist, dist + 1):\n",
    "                for dy in range(-dist, dist + 1):\n",
    "                    if abs(dx) + abs(dy) != dist:  # Only consider cells at exact distance\n",
    "                        continue\n",
    "                        \n",
    "                    nx, ny = source.x + dx, source.y + dy\n",
    "                    \n",
    "                    # Check bounds\n",
    "                    if 0 <= nx < height and 0 <= ny < width:\n",
    "                        # Calculate distance from source\n",
    "                        distance = abs(dx) + abs(dy)\n",
    "                        \n",
    "                        # Calculate attenuation based on distance and walls\n",
    "                        attenuation = 1.0 / (1 + decay_factor * distance + 0.1 * distance**1.5)\n",
    "                        \n",
    "                        # Find path from source to current position and calculate permeability\n",
    "                        path_attenuation = calculate_path_attenuation(source.x, source.y, nx, ny, wall_map)\n",
    "                        \n",
    "                        # Apply both distance and wall attenuations\n",
    "                        effective_attenuation = attenuation * path_attenuation\n",
    "                        \n",
    "                        # Add attenuated sound to this cell\n",
    "                        sound_map[nx, ny] += source.volume * effective_attenuation\n",
    "    \n",
    "    # Normalize the sound map to [0, 1]\n",
    "    if sound_map.max() > 0:\n",
    "        sound_map = sound_map / sound_map.max()\n",
    "    \n",
    "    return sound_map\n",
    "\n",
    "\n",
    "def calculate_path_attenuation(start_x, start_y, end_x, end_y, wall_map):\n",
    "    \"\"\"\n",
    "    Calculate the attenuation along the path from start to end based on walls.\n",
    "    Uses a simple approach considering the direct path.\n",
    "    \n",
    "    Args:\n",
    "        start_x, start_y: Starting coordinates\n",
    "        end_x, end_y: Ending coordinates\n",
    "        wall_map: 2D array with permeability values for each cell\n",
    "        \n",
    "    Returns:\n",
    "        Combined permeability factor for the path\n",
    "    \"\"\"\n",
    "    # For simplicity, we'll use Bresenham's line algorithm concept to find path\n",
    "    # and multiply the permeabilities of cells in the path\n",
    "    \n",
    "    dx = abs(end_x - start_x)\n",
    "    dy = abs(end_y - start_y)\n",
    "    \n",
    "    # Simple approximation: average permeability along the primary directions\n",
    "    min_x, max_x = min(start_x, end_x), max(start_x, end_x)\n",
    "    min_y, max_y = min(start_y, end_y), max(start_y, end_y)\n",
    "    \n",
    "    combined_permeability = 1.0\n",
    "    path_length = 0\n",
    "    \n",
    "    # Average permeability along x direction\n",
    "    if dx > 0:\n",
    "        for x in range(min_x, max_x + 1):\n",
    "            avg_permeability = wall_map[x, start_y] if start_y == end_y else (wall_map[x, start_y] + wall_map[x, end_y]) / 2\n",
    "            combined_permeability *= avg_permeability\n",
    "            path_length += 1\n",
    "            \n",
    "    # Average permeability along y direction\n",
    "    if dy > 0:\n",
    "        for y in range(min_y, max_y + 1):\n",
    "            avg_permeability = wall_map[start_x, y] if start_x == end_x else (wall_map[start_x, y] + wall_map[end_x, y]) / 2\n",
    "            combined_permeability *= avg_permeability\n",
    "            path_length += 1\n",
    "    \n",
    "    # If no path considered, return 1.0 (no attenuation)\n",
    "    if path_length == 0:\n",
    "        return 1.0\n",
    "        \n",
    "    # Take the geometric mean to avoid overly small values\n",
    "    return combined_permeability ** (1.0 / path_length) if path_length > 0 else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/colab_visualization.py\n",
    "\"\"\"\n",
    "Visualization module for Google Colab that uses matplotlib instead of pygame.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColabVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizer for the sound navigation environment using matplotlib.\n",
    "    Designed specifically for Google Colab compatibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the visualizer.\"\"\"\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        \n",
    "    def update(self, env):\n",
    "        \"\"\"\n",
    "        Update visualization with current environment state.\n",
    "        \n",
    "        Args:\n",
    "            env: Environment object with grid, agent_pos, and sound_sources\n",
    "        \"\"\"\n",
    "        # Create or clear the plot\n",
    "        if self.fig is None:\n",
    "            self.fig, self.ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Clear the axes\n",
    "        self.ax.clear()\n",
    "        \n",
    "        # Create a copy of the grid for visualization\n",
    "        vis_grid = env.grid.copy()\n",
    "        \n",
    "        # Mark the agent position if exists\n",
    "        if env.agent_pos is not None:\n",
    "            x, y = env.agent_pos\n",
    "            vis_grid[x][y] = 2\n",
    "            \n",
    "        # Mark sound sources if exist\n",
    "        for source in env.sound_sources:\n",
    "            vis_grid[source.x][source.y] = 3\n",
    "        \n",
    "        # Display the grid\n",
    "        im = self.ax.imshow(vis_grid, cmap='viridis', interpolation='nearest')\n",
    "        self.ax.set_title(f'Grid World Visualization - Step: {getattr(env, \"step_count\", 0)}')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = self.fig.colorbar(im, ax=self.ax, label='Cell Type (0: Empty, 1: Wall, 2: Agent, 3: Sound Source)')\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close the visualization.\"\"\"\n",
    "        if self.fig is not None:\n",
    "            plt.close(self.fig)\n",
    "\n",
    "\n",
    "def visualize_single_frame(env, title=\"Environment State\"):\n",
    "    \"\"\"\n",
    "    Visualize a single frame of the environment without maintaining state.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment object with grid, agent_pos, and sound_sources\n",
    "        title: Title for the visualization\n",
    "    \"\"\"\n",
    "    # Create a copy of the grid for visualization\n",
    "    vis_grid = env.grid.copy()\n",
    "    \n",
    "    # Mark the agent position if exists\n",
    "    if env.agent_pos is not None:\n",
    "        x, y = env.agent_pos\n",
    "        vis_grid[x][y] = 2\n",
    "    \n",
    "    # Mark sound sources if exist\n",
    "    for source in env.sound_sources:\n",
    "        vis_grid[source.x][source.y] = 3\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(vis_grid, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Cell Type (0: Empty, 1: Wall, 2: Agent, 3: Sound Source)')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_progress(losses, rewards):\n",
    "    \"\"\"\n",
    "    Plot training progress showing losses and rewards over time.\n",
    "    \n",
    "    Args:\n",
    "        losses: List of loss values during training\n",
    "        rewards: List of cumulative rewards during training\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(losses)\n",
    "    ax1.set_title('Training Loss Over Time')\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax2.plot(rewards)\n",
    "    ax2.set_title('Cumulative Reward Over Episodes')\n",
    "    ax2.set_xlabel('Episodes')\n",
    "    ax2.set_ylabel('Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/audio_processing.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_audio_observation_features(sound_intensity, frequency_content):\n",
    "    \"\"\"\n",
    "    Extract audio features from the sound at the agent's position.\n",
    "    \n",
    "    Args:\n",
    "        sound_intensity: Intensity of sound at the agent's position\n",
    "        frequency_content: Dominant frequency at the agent's position\n",
    "        \n",
    "    Returns:\n",
    "        Array of audio features\n",
    "    \"\"\"\n",
    "    # Basic features: intensity, frequency, and their combinations\n",
    "    features = [\n",
    "        sound_intensity,           # Overall intensity\n",
    "        frequency_content,         # Dominant frequency\n",
    "        sound_intensity ** 2,      # Quadratic term for intensity\n",
    "        frequency_content ** 2,    # Quadratic term for frequency\n",
    "        sound_intensity * frequency_content,  # Cross-term\n",
    "        np.sqrt(max(0, sound_intensity)),     # Square root of intensity\n",
    "        np.sin(2 * np.pi * frequency_content),  # Sinusoidal transformation of frequency\n",
    "        np.cos(2 * np.pi * frequency_content),  # Cosine transformation of frequency\n",
    "    ]\n",
    "    \n",
    "    # Add some basic directional features based on intensity changes\n",
    "    # These would normally come from comparing with previous observations\n",
    "    # For now, we'll include placeholder values\n",
    "    features.extend([0.0, 0.0, 0.0, 0.0])  # Placeholder for directional features\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def extract_spectral_features(audio_signal, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Extract spectral features from an audio signal.\n",
    "    \n",
    "    Args:\n",
    "        audio_signal: Array representing the audio signal\n",
    "        sample_rate: Sample rate of the audio signal\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of spectral features\n",
    "    \"\"\"\n",
    "    # Compute FFT\n",
    "    fft = np.fft.fft(audio_signal)\n",
    "    magnitude_spectrum = np.abs(fft[:len(fft)//2])\n",
    "    \n",
    "    # Compute spectral centroid (center of mass of spectrum)\n",
    "    frequencies = np.arange(len(magnitude_spectrum)) * (sample_rate / len(audio_signal))\n",
    "    spectral_centroid = np.sum(frequencies * magnitude_spectrum) / np.sum(magnitude_spectrum)\n",
    "    \n",
    "    # Compute spectral rolloff (frequency below which a certain percentage of energy is contained)\n",
    "    threshold = 0.85 * np.sum(magnitude_spectrum)\n",
    "    spectral_rolloff = frequencies[np.argmax(np.cumsum(magnitude_spectrum) >= threshold)]\n",
    "    \n",
    "    return {\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'spectral_rolloff': spectral_rolloff,\n",
    "        'power': np.mean(audio_signal ** 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/environment_gen.py\n",
    "import numpy as np\n",
    "from core.grid_world import GridWorld, Agent\n",
    "from core.sound_source import SoundSource, Wall\n",
    "from core.tasks import create_task_environment\n",
    "\n",
    "\n",
    "def generate_random_environment(task_type: int, width: int = 25, height: int = 25):\n",
    "    \"\"\"\n",
    "    Generate a random environment for the specified task.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (1, 2, or 3)\n",
    "        width: Width of the grid\n",
    "        height: Height of the grid\n",
    "        \n",
    "    Returns:\n",
    "        Environment object for the specified task\n",
    "    \"\"\"\n",
    "    # Create base grid world\n",
    "    env = create_task_environment(task_type, width, height)\n",
    "    \n",
    "    # Randomly place walls (about 10-20% of cells)\n",
    "    num_walls = int(0.15 * width * height)\n",
    "    for _ in range(num_walls):\n",
    "        x, y = np.random.randint(0, width), np.random.randint(0, height)\n",
    "        if (x, y) != env.agent_pos and (x, y) not in [(s.x, s.y) for s in env.sound_sources]:\n",
    "            permeability = np.random.uniform(0.1, 0.9)  # Random permeability\n",
    "            env.place_wall(x, y, permeability)\n",
    "    \n",
    "    # Make sure agent position is properly set\n",
    "    if env.agent_pos is None:\n",
    "        # Find an empty spot for the agent\n",
    "        empty_cells = []\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                if env.grid[x][y] == 0:  # Empty cell\n",
    "                    empty_cells.append((x, y))\n",
    "        \n",
    "        if empty_cells:\n",
    "            agent_x, agent_y = empty_cells[np.random.choice(len(empty_cells))]\n",
    "            env.place_agent(agent_x, agent_y)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def manual_environment_setup(task_type: int):\n",
    "    \"\"\"\n",
    "    Set up an environment manually based on user input.\n",
    "    For Colab compatibility, we'll create a simple predefined environment.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (1, 2, or 3)\n",
    "        \n",
    "    Returns:\n",
    "        Environment object for the specified task\n",
    "    \"\"\"\n",
    "    print(f\"Creating manual environment for Task {task_type}\")\n",
    "    \n",
    "    # Create base environment for the task\n",
    "    env = create_task_environment(task_type, 25, 25)\n",
    "    \n",
    "    # Add some predefined walls\n",
    "    wall_positions = [\n",
    "        (5, 5), (5, 6), (5, 7),\n",
    "        (10, 15), (11, 15), (12, 15),\n",
    "        (15, 10), (15, 11), (15, 12), (15, 13)\n",
    "    ]\n",
    "    \n",
    "    for x, y in wall_positions:\n",
    "        env.place_wall(x, y, permeability=np.random.uniform(0.2, 0.8))\n",
    "    \n",
    "    # Ensure agent is placed if not already\n",
    "    if env.agent_pos is None:\n",
    "        env.place_agent(2, 2)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile core/tasks.py\n",
    "import numpy as np\n",
    "from core.grid_world import GridWorld, Agent\n",
    "from core.sound_source import SoundSource\n",
    "\n",
    "\n",
    "class BaseTaskEnvironment(GridWorld):\n",
    "    \"\"\"\n",
    "    Base class for task-specific environments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width=25, height=25, max_steps=500):\n",
    "        super().__init__(width, height)\n",
    "        self.max_steps = max_steps\n",
    "        self.step_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        self.agent = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        super().reset()\n",
    "        self.step_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: Action to take\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (observation, reward, done)\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Environment is done. Please reset before continuing.\")\n",
    "        \n",
    "        # Move the agent\n",
    "        old_pos = self.agent_pos\n",
    "        new_pos = self.agent.move(action, self)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self.compute_reward(old_pos, new_pos, action)\n",
    "        \n",
    "        # Update step count\n",
    "        self.step_count += 1\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Check termination conditions\n",
    "        self.done = self.check_done()\n",
    "        \n",
    "        # Get new observation\n",
    "        sound_map = self.compute_sound_map()\n",
    "        observation = self.agent.observe(sound_map=sound_map, grid_world=self)\n",
    "        \n",
    "        return observation, reward, self.done\n",
    "    \n",
    "    def compute_reward(self, old_pos, new_pos, action):\n",
    "        \"\"\"\n",
    "        Compute reward for the transition from old_pos to new_pos.\n",
    "        This method should be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses should implement compute_reward method\")\n",
    "    \n",
    "    def check_done(self):\n",
    "        \"\"\"\n",
    "        Check if the episode is done.\n",
    "        \"\"\"\n",
    "        return self.step_count >= self.max_steps\n",
    "    \n",
    "    \n",
    "class FindAllSourcesTask(BaseTaskEnvironment):\n",
    "    \"\"\"\n",
    "    Task: Find all sound sources in the environment.\n",
    "    Reward: Positive reward for finding each new source, small penalty for each step.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width=25, height=25, max_steps=500):\n",
    "        super().__init__(width, height, max_steps)\n",
    "        self.found_sources = set()\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.found_sources = set()\n",
    "        \n",
    "    def compute_reward(self, old_pos, new_pos, action):\n",
    "        \"\"\"\n",
    "        Reward is positive when finding a new source, negative for each step.\n",
    "        \"\"\"\n",
    "        # Small negative reward for each step to encourage efficiency\n",
    "        reward = -0.01\n",
    "        \n",
    "        # Check if the agent is at the same position as any unfound sound source\n",
    "        for i, source in enumerate(self.sound_sources):\n",
    "            if new_pos == (source.x, source.y) and i not in self.found_sources:\n",
    "                self.found_sources.add(i)\n",
    "                reward += 1.0  # Positive reward for finding a new source\n",
    "                \n",
    "        return reward\n",
    "        \n",
    "    def check_done(self):\n",
    "        \"\"\"\n",
    "        Episode is done when all sources are found or max steps reached.\n",
    "        \"\"\"\n",
    "        return len(self.found_sources) == len(self.sound_sources) or self.step_count >= self.max_steps\n",
    "\n",
    "\n",
    "class FindQuietestPlaceTask(BaseTaskEnvironment):\n",
    "    \"\"\"\n",
    "    Task: Find the quietest place in the environment.\n",
    "    Reward: Based on how quiet the current location is compared to others.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width=25, height=25, max_steps=500):\n",
    "        super().__init__(width, height, max_steps)\n",
    "        self.sound_map_history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.sound_map_history = []\n",
    "        \n",
    "    def compute_reward(self, old_pos, new_pos, action):\n",
    "        \"\"\"\n",
    "        Reward is based on how much quieter the new position is compared to previous positions.\n",
    "        \"\"\"\n",
    "        # Compute current sound map\n",
    "        sound_map = self.compute_sound_map()\n",
    "        current_intensity = sound_map[new_pos[0]][new_pos[1]]\n",
    "        \n",
    "        # Negative reward proportional to sound intensity (quieter = higher reward)\n",
    "        reward = -current_intensity\n",
    "        \n",
    "        # Additional reward if this is the quietest place found so far\n",
    "        if not self.sound_map_history or current_intensity < min(self.sound_map_history):\n",
    "            self.sound_map_history.append(current_intensity)\n",
    "            reward += 0.5  # Bonus for finding a quieter place\n",
    "            \n",
    "        # Small penalty for each step\n",
    "        reward -= 0.01\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "class FollowMovingSourceTask(BaseTaskEnvironment):\n",
    "    \"\"\"\n",
    "    Task: Follow a moving sound source.\n",
    "    The sound source moves randomly around the environment.\n",
    "    Reward: Positive when close to the source, negative when far.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width=25, height=25, max_steps=500):\n",
    "        super().__init__(width, height, max_steps)\n",
    "        self.moving_source_idx = 0  # Index of the moving source\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        # Reset the moving source to its original position\n",
    "        if hasattr(self, '_original_source_positions'):\n",
    "            for i, pos in enumerate(self._original_source_positions):\n",
    "                if i < len(self.sound_sources):\n",
    "                    self.sound_sources[i].x = pos[0]\n",
    "                    self.sound_sources[i].y = pos[1]\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment, including moving the sound source.\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Environment is done. Please reset before continuing.\")\n",
    "        \n",
    "        # Move the agent\n",
    "        old_pos = self.agent_pos\n",
    "        new_pos = self.agent.move(action, self)\n",
    "        \n",
    "        # Move the sound source randomly\n",
    "        self._move_sound_source_randomly()\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self.compute_reward(old_pos, new_pos, action)\n",
    "        \n",
    "        # Update step count\n",
    "        self.step_count += 1\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Check termination conditions\n",
    "        self.done = self.check_done()\n",
    "        \n",
    "        # Get new observation\n",
    "        sound_map = self.compute_sound_map()\n",
    "        observation = self.agent.observe(sound_map=sound_map, grid_world=self)\n",
    "        \n",
    "        return observation, reward, self.done\n",
    "    \n",
    "    def _move_sound_source_randomly(self):\n",
    "        \"\"\"Move one of the sound sources randomly.\"\"\"\n",
    "        if self.sound_sources:\n",
    "            # Select a source to move (could be randomized selection)\n",
    "            source = self.sound_sources[self.moving_source_idx % len(self.sound_sources)]\n",
    "            \n",
    "            # Possible moves: up, down, left, right, stay\n",
    "            moves = [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)]\n",
    "            move = moves[np.random.choice(len(moves))]\n",
    "            \n",
    "            new_x = source.x + move[0]\n",
    "            new_y = source.y + move[1]\n",
    "            \n",
    "            # Check bounds and make sure it's not a wall\n",
    "            if (0 <= new_x < self.width and 0 <= new_y < self.height and \n",
    "                self.grid[new_x][new_y] != 1):  # Not a wall\n",
    "                source.x = new_x\n",
    "                source.y = new_y\n",
    "                \n",
    "            # Move to next source for next step\n",
    "            self.moving_source_idx += 1\n",
    "    \n",
    "    def compute_reward(self, old_pos, new_pos, action):\n",
    "        \"\"\"\n",
    "        Reward based on proximity to the closest sound source.\n",
    "        \"\"\"\n",
    "        # Find the closest sound source\n",
    "        min_distance = float('inf')\n",
    "        for source in self.sound_sources:\n",
    "            distance = abs(new_pos[0] - source.x) + abs(new_pos[1] - source.y)  # Manhattan distance\n",
    "            min_distance = min(min_distance, distance)\n",
    "        \n",
    "        # Reward based on proximity (closer = higher reward)\n",
    "        # Using inverse relationship with distance\n",
    "        if min_distance == 0:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 1.0 / (min_distance + 0.1)\n",
    "            \n",
    "        # Small penalty for each step\n",
    "        reward -= 0.01\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "def create_task_environment(task_type, width=25, height=25):\n",
    "    \"\"\"\n",
    "    Factory function to create task-specific environments.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (1, 2, or 3)\n",
    "        width: Width of the grid\n",
    "        height: Height of the grid\n",
    "        \n",
    "    Returns:\n",
    "        Task-specific environment instance\n",
    "    \"\"\"\n",
    "    if task_type == 1:\n",
    "        env = FindAllSourcesTask(width, height)\n",
    "        \n",
    "        # Add a few sound sources\n",
    "        env.place_sound_source(SoundSource(5, 5, volume=0.8))\n",
    "        env.place_sound_source(SoundSource(20, 20, volume=0.9))\n",
    "        env.place_sound_source(SoundSource(10, 15, volume=0.7))\n",
    "        \n",
    "        # Place agent\n",
    "        env.place_agent(2, 2)\n",
    "        env.agent = Agent(2, 2)\n",
    "        \n",
    "    elif task_type == 2:\n",
    "        env = FindQuietestPlaceTask(width, height)\n",
    "        \n",
    "        # Add sound sources positioned to create gradient of sound\n",
    "        env.place_sound_source(SoundSource(3, 3, volume=1.0))\n",
    "        env.place_sound_source(SoundSource(22, 3, volume=1.0))\n",
    "        env.place_sound_source(SoundSource(3, 22, volume=1.0))\n",
    "        env.place_sound_source(SoundSource(22, 22, volume=1.0))\n",
    "        \n",
    "        # Place agent\n",
    "        env.place_agent(12, 12)  # Center position\n",
    "        env.agent = Agent(12, 12)\n",
    "        \n",
    "    elif task_type == 3:\n",
    "        env = FollowMovingSourceTask(width, height)\n",
    "        \n",
    "        # Add a primary source that moves and a few stationary ones\n",
    "        env.place_sound_source(SoundSource(5, 5, volume=0.9))  # Moving source\n",
    "        env.place_sound_source(SoundSource(15, 15, volume=0.6))\n",
    "        env.place_sound_source(SoundSource(20, 5, volume=0.7))\n",
    "        \n",
    "        # Store original positions for reset\n",
    "        env._original_source_positions = [(s.x, s.y) for s in env.sound_sources]\n",
    "        \n",
    "        # Place agent\n",
    "        env.place_agent(2, 2)\n",
    "        env.agent = Agent(2, 2)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rl/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rl/dqn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# Check if CUDA is available\n",
    "TORCH_AVAILABLE = torch.cuda.is_available() or torch.backends.mps.is_available()\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, hidden_sizes: list = None):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of the input layer\n",
    "            output_size: Size of the output layer (number of actions)\n",
    "            hidden_sizes: List of sizes for hidden layers (default: [64, 64])\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [64, 64]\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor with Q-values for each action\n",
    "        \"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        \n",
    "        # Ensure x has the right shape\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer for storing and sampling experiences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum capacity of the buffer\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state after action\n",
    "            done: Whether the episode is done\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of experiences to sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgentWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for the DQN agent that handles training and inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, lr: float = 0.001, \n",
    "                 gamma: float = 0.99, epsilon: float = 1.0, epsilon_decay: float = 0.995,\n",
    "                 epsilon_min: float = 0.01, buffer_size: int = 10000, \n",
    "                 target_update_freq: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of the input observations\n",
    "            output_size: Number of possible actions\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: Initial exploration rate\n",
    "            epsilon_decay: Rate at which epsilon decreases\n",
    "            epsilon_min: Minimum exploration rate\n",
    "            buffer_size: Size of the replay buffer\n",
    "            target_update_freq: How often to update the target network\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQN(input_size, output_size).to(self.device)\n",
    "        self.target_network = DQN(input_size, output_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Sync target network with main network\n",
    "        self.sync_target_network()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def sync_target_network(self):\n",
    "        \"\"\"Sync the target network with the main network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state after action\n",
    "            done: Whether the episode is done\n",
    "        \"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def act(self, state, training: bool = True):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            training: Whether the agent is in training mode (affects epsilon-greedy policy)\n",
    "            \n",
    "        Returns:\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        # Decay epsilon during training\n",
    "        if training:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() <= self.epsilon and training:\n",
    "            return np.random.choice(self.output_size)\n",
    "        \n",
    "        # Convert state to tensor and get Q-values\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        # Return the action with highest Q-value\n",
    "        return np.argmax(q_values.cpu().data.numpy())\n",
    "        \n",
    "    def replay(self, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Train the agent on a batch of experiences from the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Size of the batch to train on\n",
    "        \n",
    "        Returns:\n",
    "            Loss value for this batch\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return 0.0  # Not enough samples to train\n",
    "        \n",
    "        # Sample experiences from the buffer\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.BoolTensor(dones).to(self.device)\n",
    "        \n",
    "        # Get current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Get next Q-values from target network\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update step count and periodically sync target network\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update_freq == 0:\n",
    "            self.sync_target_network()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to save the model\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'step_count': self.step_count\n",
    "        }, filepath)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to load the model from\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Model file does not exist: {filepath}\")\n",
    "            return\n",
    "        \n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.step_count = checkpoint['step_count']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rl/training.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from rl.dqn import DQNAgentWrapper\n",
    "from utils.environment_gen import generate_random_environment\n",
    "from core.tasks import create_task_environment\n",
    "\n",
    "\n",
    "def train_task(task_type: int, num_episodes: int = 1000, \n",
    "               model_path: str = None, lr: float = 0.001,\n",
    "               epsilon_decay: float = 0.995, batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Train a DQN agent for a specific task.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (1, 2, or 3)\n",
    "        num_episodes: Number of episodes to train for\n",
    "        model_path: Path to save/load model\n",
    "        lr: Learning rate for the optimizer\n",
    "        epsilon_decay: Rate at which exploration rate decreases\n",
    "        batch_size: Size of the training batches\n",
    "        \n",
    "    Returns:\n",
    "        Trained agent and list of losses\n",
    "    \"\"\"\n",
    "    # Create environment for the task\n",
    "    env = create_task_environment(task_type)\n",
    "    \n",
    "    # Determine observation size by getting a sample observation\n",
    "    from utils.audio_processing import get_audio_observation_features\n",
    "    sample_obs = get_audio_observation_features(0.5, 0.5)\n",
    "    obs_size = len(sample_obs)\n",
    "    action_size = 5  # up, down, left, right, stay\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgentWrapper(\n",
    "        input_size=obs_size,\n",
    "        output_size=action_size,\n",
    "        lr=lr,\n",
    "        epsilon_decay=epsilon_decay\n",
    "    )\n",
    "    \n",
    "    # Lists to store training metrics\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    print(f\"Starting training for Task {task_type} with {num_episodes} episodes\")\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(num_episodes), desc=f\"Training Task {task_type}\"):\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        obs = env.agent.observe(sound_map=env.compute_sound_map(), grid_world=env)\n",
    "        \n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not env.done and step_count < env.max_steps:\n",
    "            # Select action\n",
    "            action = agent.act(obs, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            \n",
    "            # Remember experience\n",
    "            agent.remember(obs, action, reward, next_obs, done)\n",
    "            \n",
    "            # Train on batch\n",
    "            loss = agent.replay(batch_size)\n",
    "            if loss > 0:\n",
    "                episode_loss += loss\n",
    "            \n",
    "            # Update state\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "        \n",
    "        # Store metrics\n",
    "        losses.append(episode_loss / max(step_count, 1))\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress occasionally\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    # Save model if path provided\n",
    "    if model_path:\n",
    "        agent.save(model_path)\n",
    "    \n",
    "    print(f\"Training completed for Task {task_type}\")\n",
    "    return agent, losses\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, task_type, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent on a specific task.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained DQNAgentWrapper\n",
    "        task_type: Type of task (1, 2, or 3)\n",
    "        num_episodes: Number of episodes to evaluate for\n",
    "        \n",
    "    Returns:\n",
    "        Average reward across episodes\n",
    "    \"\"\"\n",
    "    # Create environment for the task\n",
    "    env = create_task_environment(task_type)\n",
    "    \n",
    "    # Set agent to evaluation mode (no exploration)\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0  # No exploration during evaluation\n",
    "    \n",
    "    total_rewards = []\n",
    "    steps_counts = []\n",
    "    \n",
    "    print(f\"Evaluating agent for Task {task_type} with {num_episodes} episodes\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        obs = env.agent.observe(sound_map=env.compute_sound_map(), grid_world=env)\n",
    "        \n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not env.done and step_count < env.max_steps:\n",
    "            # Select action (no exploration)\n",
    "            action = agent.act(obs, training=False)\n",
    "            \n",
    "            # Take action\n",
    "            obs, reward, done = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        steps_counts.append(step_count)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}, Steps = {step_count}\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_steps = np.mean(steps_counts)\n",
    "    \n",
    "    print(f\"Evaluation completed for Task {task_type}\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f}, Average Steps: {avg_steps:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "def train_all_tasks(num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train agents for all tasks sequentially.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes: Number of episodes to train each agent for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping task types to trained agents\n",
    "    \"\"\"\n",
    "    agents = {}\n",
    "    \n",
    "    for task_type in [1, 2, 3]:\n",
    "        print(f\"\\n--- Training for Task {task_type} ---\")\n",
    "        \n",
    "        # Create model path\n",
    "        model_dir = \"models\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_path = f\"{model_dir}/dqn_task_{task_type}.pth\"\n",
    "        \n",
    "        # Train the agent\n",
    "        agent, losses = train_task(\n",
    "            task_type=task_type,\n",
    "            num_episodes=num_episodes,\n",
    "            model_path=model_path\n",
    "        )\n",
    "        \n",
    "        agents[task_type] = agent\n",
    "        \n",
    "        print(f\"Task {task_type} training completed\")\n",
    "    \n",
    "    print(\"\\nAll tasks training completed!\")\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    \n",
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('./core')\n",
    "sys.path.append('./rl')\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \n",
    "from rl.training import train_task, evaluate_agent\n",
    "from utils.environment_gen import generate_random_environment\n",
    "\n",
    "print(\"     1...\")\n",
    "agent, losses = train_task(\n",
    "    task_type=1,\n",
    "    num_episodes=50,  #      Colab\n",
    "    model_path=None\n",
    ")\n",
    "\n",
    "print(\"  ...\")\n",
    "evaluate_agent(agent, task_type=1, num_episodes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}